\chapter{Convex optimization problems}

% 4.1
\section{Optimization problems}

% 4.1.1
\subsection{Basic terminology}
\label{subsec:4.1.1}
Problem notation
\begin{align}
  \text{minimize\;\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& f_i(x)\le 0,\;i=1,\dots,m\label{eq:4.1}\\
                          & h_i(x) =  0,\;i=1,\dots,p\nonumber
\end{align}
\begin{itemize}
  \item \textit{optimization variable}: $x\in\R^n$
  \item \textit{objective/cost function}: $f_0:\R^n\rightarrow\R$
  \item \textit{inequality constraints function}: $f_i(x):\R^n\rightarrow\R$
  \item \textit{equality constraints function}: $h_i(x):\R^n\rightarrow\R$
  \item if there are no constraints, we say problem \eqref{eq:4.1} is \textit{unconstrained}
  \item domain of problem \eqref{eq:4.1}: $\D=\bigcap_{i=0}^m\dom f_i\;\cap\;\bigcap_{i=0}^p\dom h_i$
  \item a point $x\in\D$ is \textit{feasible} if it satisfies all constraints
  \item \textit{feasible/constraint set}: the set of all feasible points
\end{itemize}
The \textit{optimal value} $p^\ast$ is defined by
\begin{align*}
  p^\ast=\inf\{f_0(x)\mid f_i(x)\le 0,\;i=1,\dots,m,\;h_i(x)=0,\;i=1,\dots,p\}
\end{align*}
\begin{itemize}
  \item allow $p^\ast$ take on extend values $\pm\infty$
  \item if the problem is \textit{infeasible}, then $p^\ast=\inf\{\varnothing\}=\infty$
  \item if the problem is \textit{unbounded below}, then there are feasible points $x_k$ with $f_0(x_k)\rightarrow -\infty$ as $k\rightarrow\infty$, $p^\ast=-\infty$
\end{itemize}
\subsubsection{Optimal and locally optimal points}
The \textit{optimal point} $x^\ast$: $x^\ast$ is feasible and $f_0(x)=p^\ast$.
Hence, \textit{optimal set} is denoted by
\begin{align*}
  X_{\text{opt}}=\{x\mid f_i(x)\le 0,\;i=1,\dots,m,\;h_i(x)=0,\;i=1,\dots,p,\;f_0(x)=p^\ast\}
\end{align*}
\begin{itemize}
  \item if there exists optimal point in \eqref{eq:4.1}, then the optimal value is \textit{attained} or \textit{achieved}
  \item the problem is then \textit{solvable}
\end{itemize}
A feasible point $x$ with $f_0(x)\le p^\ast+\epsilon$, $\epsilon>0$ is called $\epsilon$-\textit{suboptimal}, and the set of all $\epsilon$-\textit{suboptimal} points is called the $\epsilon$-\textit{suboptimal set}.\par
A point $x$ is \textit{locally optimal} if there is an $R>0$ such that
\begin{align*}
  f_0(x)=\inf\{f_0(z)\mid f_i(z)\le 0,\;i=1,\dots,m,\;h_i(z)=0,\;i=1,\dots,p,\;\|z-x\|_2^2\le R\}
\end{align*}
or, $x$ solves the problem
\begin{align*}
  \text{minimize\;\;}\quad& f_0(z)\\
  \text{subject to}  \quad& f_i(z)\le 0,\;i=1,\dots,m\\
                          & h_i(z) =  0,\;i=1,\dots,p\\
                          & \|z-x\|_2^2\le R
\end{align*}
\begin{itemize}
  \item if $x$ is feasible and $f_i(x)=0$, then the constraint is \textit{active} at $x$
  \item if $x$ is feasible and $f_i(x)<0$, then the constraint is \textit{inactive} at $x$
  \item if $x$ is feasible, then all the equality constraints are \textit{active}
  \item a constraint is \textit{redundant} if deleting it does not change the feasible set
\end{itemize}
\begin{example}
  A few simple unconstrained problems with $x\in\R$ and $\dom f_0=\R_{++}$
  \begin{itemize}
    \item $f_0(x)=1/x$: $p^\ast=0$, but the optimal value is not achieved ($x^\ast$ does not exist)
    \item $f_0(x)=-\log x$: $p^\ast=-\infty$, the problem is unbounded below
    \item $f_0(x)=x\log x$: $p^\ast=-1/e$ with unique optimal point $x^\ast=1/e$
  \end{itemize}
\end{example}
\subsubsection{Feasibility problems}
If the objective function is identically zero, then $p^\ast$ is either $0$ or $\infty$ (if feasible set is empty). The \textit{feasibility problem} is
\begin{align*}
  \text{find\;\;\;\;\;\;}\quad\quad& x\\
  \text{subject to}      \quad& f_i(x)\le 0,\;i=1,\dots,m\\
                              & h_i(x) =  0,\;i=1,\dots,p\\
\end{align*}
The problem is to determine whether the constraints are consistent.

% 4.1.2
\subsection{Expressing problems in standard form}
We refer to \eqref{eq:4.1} as an optimization problem in \text{standard form}.
\begin{example}[\textit{Box constraints}]
  The problem
  \begin{align*}
    \text{minimize\;\;}\quad& f_0(x)\\
    \text{subject to}  \quad& l_i\le x_i\le u_i,\;i=1,\dots,n
  \end{align*}
  where $x\in\R^n$. Its feasible set is a box, and the standard form is
  \begin{align*}
    \text{minimize\;\;}\quad& f_0(x)\\
    \text{subject to}  \quad& l_i-x_i\le 0,\;i=1,\dots,n\\
                            & x_i-u_i\le 0,\;i=1,\dots,n
  \end{align*}
\end{example}
\subsubsection{Maximization problems}
The maximization problems can be solved by minimizing $-f_0$ and have the form
\begin{align}
  \text{maximize\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& f_i(x)\le 0,\;i=1,\dots,m\label{eq:4.2}\\
                          & h_i(x) =  0,\;i=1,\dots,p\nonumber
\end{align}
\begin{itemize}
  \item optimal value: $p^\ast=\sup\{f_0(x)\mid f_i(x)\le 0,\;i=1,\dots,m,\;h_i(x)=0,\;i=1,\dots,p\}$
  \item a feasible point $x$ is $\epsilon$-suboptimal if $f_0(x)\ge p^\ast-\epsilon$
\end{itemize}

% 4.1.3
\subsection{Equivalent problems}
Informally, we call two problems \textit{equivalent} if from a solution of one, a solution of the other is readily found, and vice versa.
A simple example,
\begin{align}
  \text{minimize\;\;}\quad& \tilde{f}(x)  =\alpha_0f_0(x)\nonumber\\
  \text{subject to}  \quad& \tilde{f}_i(x)=\alpha_if_i(x)\le 0,\;i=1,\dots,m\label{eq:4.3}\\
                          & \tilde{h}_i(x)=\beta_ih_i(x)  =  0,\;i=1,\dots,p\nonumber
\end{align}
where $\alpha_i>0$ and $\beta_i\neq 0$.\\
\eqref{eq:4.1} and \eqref{eq:4.3} are equivalent but not the same.
\subsubsection{Change of variables}
Suppose $\phi:\R^n\rightarrow\R^n$ is one-to-one with $\phi(\dom\phi)\supseteq\D$ (domain of \ref{eq4_1}).
Define $\tilde{f}_i$ and $\tilde{h}_i$ as
\begin{align*}
  \tilde{f}_i(z)=f_i(\phi(z)),\;i=0,\dots,m\quad\quad\tilde{h}_i(z)=h_i(\phi(z)),\;i=1,\dots,p
\end{align*}
Consider the problem
\begin{align}
  \text{minimize\;\;}\quad& \tilde{f}_0(x)\nonumber\\
  \text{subject to}  \quad& \tilde{f}_i(x)\le 0,\;i=1,\dots,m\label{eq:4.4}\\
                          & \tilde{h}_i(x) =  0,\;i=1,\dots,p\nonumber
\end{align}
with variable $z$. Then \eqref{eq:4.1} and \eqref{eq:4.4} are equivalent.
\subsubsection{Transformation of objective and constraint functions}
Suppose that
\begin{enumerate}
  \item $\psi_0:\R\rightarrow\R$ is monotone increasing.
  \item $\psi_1,\dots,\psi_m:\R\rightarrow\R$ satisfy $\psi_i(u)\le 0\Leftrightarrow u\le 0$.
  \item $\psi_{m+1},\dots,\psi_{m+p}:\R\rightarrow\R$ satisfy $\psi_i(u)=0\Leftrightarrow u=0$.
\end{enumerate}
Define $\tilde{f}_i$ and $\tilde{h}_i$ as
\begin{align*}
  \tilde{f}_i(x)=\psi_i(f_i(x)),\;i=0,\dots,m\quad\quad\tilde{h}_i(x)=\psi_{m+i}(h_i(x)),\;i=1,\dots,p
\end{align*}
The problem
\begin{align*}
  \text{minimize\;\;}\quad& \tilde{f}_0(x)\\
  \text{subject to}  \quad& \tilde{f}_i(x)\le 0,\;i=1,\dots,m\\
                          & \tilde{h}_i(x) =  0,\;i=1,\dots,p
\end{align*}
and \eqref{eq:4.1} are equivalent. This is the general rule of \eqref{eq:4.3}.
\begin{example}[\textit{Least-norm and least-norm-squared prolems}]
  Consider an unconstrained Euclidean norm minimization problem
  \begin{align}
    \text{minimize\;\;}\quad& \|Ax-b\|_2\label{eq:4.5}
  \end{align}
  with variable $x\in\R^n$. Consider another problem
  \begin{align}
    \text{minimize\;\;}\quad& \|Ax-b\|_2^2=(Ax-b)^T(Ax-b)\label{eq:4.6}
  \end{align}
  The problems \eqref{eq:4.5} and \eqref{eq:4.6} are equivalent but not the same.\par
  For example, $\|Ax-b\|_2$ is not differentiable at any $x$ with $Ax-b=0$, whereas $\|Ax-b\|_2^2$ is differentiable for all $x$.
\end{example}
\subsubsection{Slack variables}
There exists $s_i\ge 0$ that satisfies $f_i(x)+s_i=0$, and \eqref{eq:4.1} is equivalent to the problem
\begin{align}
  \text{minimize\;\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& s_i\ge 0,\;i=1,\dots,m\nonumber\\
                          & f_i(x)+s_i=0,\;i=1,\dots,m\label{eq:4.7}\\
                          & h_i(x)=0,\;i=1,\dots,p\nonumber
\end{align}
\begin{itemize}
  \item variables: $n\rightarrow n+m$
  \item inequality constraints: $m\rightarrow m$
  \item equality constraints: $p\rightarrow p+m$
\end{itemize}

\subsubsection{Eliminating equality constraints}
\subsubsection{Eliminating linear equality constraints}
\subsubsection{Introducing equality constraints}
\subsubsection{Optimizing over some variables}
\begin{example}[\textit{Minimizing a quadratic function with constraints on some variables}]
\end{example}
\subsubsection{Epigraph problem form}
\subsubsection{Implicit and explicit constraints}

% 4.1.4
\subsection{Parameter and oracle problem descriptions}

% 4.2
\section{Convex optimization}
A \textit{convex optimization problem} has the form
\begin{align}
  \text{minimize\;\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& f_i(x)\le 0,\;i=1,\dots,m\label{eq:4.15}\\
                          & a_i^Tx =b_i,\;i=1,\dots,p\nonumber
\end{align}
Requirements:
\begin{itemize}
  \item the objective function is convex
  \item the inequality constraint functions are convex
  \item the equality constraint function $h_i(x)=a_i^Tx-b$ is affine
\end{itemize}
Properties:
\begin{itemize}
  \item its feasible set is convex (assume constraints are consistent)
  \item if $f_0$ is quasiconvex, then \eqref{eq:4.15} is a standard form \textit{quasiconvex optimization problem}
  \item for both convex and quasiconvex problems, the optimal set is convex
  \item if $f_0$ is strictly convex, then the optimal set contains at most one point
\end{itemize}
\subsubsection{Concave maximization problems}
Problem \eqref{eq:4.15} with maximizing concave function $f_0(x)$ is also a convex optimization problem.
\begin{align}
  \text{maximize\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& f_i(x)\le 0,\;i=1,\dots,m\label{eq:4.16}\\
                          & a_i^Tx =b_i,\;i=1,\dots,p\nonumber
\end{align}
\subsubsection{Abstract form convex optimization problem}

% 4.2.1
\subsection{Convex optimization problems in standard form}

% 4.2.2
\subsection{Local and global optima}
For a convex optimization problem, \textbf{any locally optimal point is also globally optimal}.
\begin{proof}
  proof ....:
  \begin{align}
    f_0(x)=\inf\{f_0(z)\mid z\;\text{feasible},\;\|z-x\|_2\le R\}
  \end{align}
  for some $R>0$
\end{proof}

% 4.2.3
\subsection{An optimality criterion for differentiable $f_0$}
Suppose $f_0$ is differentiable, we have \eqref{eq:3.2}
\begin{align}
  f_0(y)\ge f_0(x)+\nabla f_0(x)^T(y-x)\quad x,y\in\dom f_0
\end{align}
Let $X$ denote the feasible set, then $x$ is optimal if and only if $x\in X$ and
\begin{align}
  \nabla f_0(x)^T(y-x)\ge 0\quad\forall y\in X
\end{align}
\subsubsection{Proof of optimality condition}
\subsubsection{Unconstrained problems}
\subsubsection{Problems with equality constraints only}
\begin{example}[\textit{Unconstrained quadratic optimization}]
  Consider $f_0=(1/2)x^TPx+q^Tx+r$ where $P\in\symm_{+}^n$ (which makes $f_0$ convex).
  Then $x$ is a minimizer of $f_0$ if and only if
  \begin{align*}
    \nabla f_0(x)=Px+q=0
  \end{align*}
  \begin{itemize}
    \item if $q\notin\range(P)$, then there is no solution. ($f_0$ is unbounded below)
    \item if $P\succ 0$ (which make $f_0$ strictly convex), then there is a unique minimizer $x^\ast=-P^{-1}q$
    \item if $P$ is singular but $q\in\range(P)$, then $X_{\text{opt}}=-P^\dagger q+\nullspace(P)$
  \end{itemize}
\end{example}
\begin{example}[\textit{Analytic centering}]
\end{example}

% 4.2.4
\subsection{Equivalent convex problems}

% 4.2.5
\subsection{Quasiconvex optimization}

% 4.3
\section{Linear optimization problems}
A \textit{linear programming} (LP) problem has the form
\begin{align}
  \text{minimize\;\;}\quad& c^Tx+d\nonumber\\
  \text{subject to}  \quad& Gx\preceq h\label{eq:4.27}\\
                          & Ax=b\nonumber
\end{align}
where $G\in\R^{m\times n}$ and $A\in\R^{p\times n}$.
\subsubsection{Standard and inequality form linear programs}
A \textit{standard form LP} is
\begin{align}
  \text{minimize\;\;}\quad& c^Tx\nonumber\\
  \text{subject to}  \quad& Ax=b\label{eq:4.28}\\
                          & x\succeq 0\nonumber
\end{align}
A \textit{inequality form LP} is
\begin{align}
  \text{minimize\;\;}\quad& c^Tx\nonumber\\
  \text{subject to}  \quad& Ax\preceq b\label{eq:4.29}
\end{align}
\subsubsection{Converting LPs to standard form}

% 4.3.1
\subsection{Examples}

% 4.3.2
\subsection{Linear-fractional programming}
A \textit{linear-fractional program} has the form
\begin{align}
  \text{minimize\;\;}\quad& f_0(x)\nonumber\\
  \text{subject to}  \quad& Gx\preceq h\label{eq:4.32}\\
                          & Ax=b\nonumber
\end{align}
where $f_0(x)=\frac{c^Tx+d}{e^Tx+f}$ and $\dom f_0=\{x\mid e^Tx+f>0\}$.
\begin{example}[\textit{Von Neumann growth problem}]
\end{example}

% 4.4
\section{Quadratic optimization problems}
A \textit{quadratic program} (QP) has the form
\begin{align}
  \text{minimize\;\;}\quad& (1/2)x^TPx+q^Tx+r\nonumber\\
  \text{subject to}  \quad& Gx\preceq h\label{eq:4.34}\\
                          & Ax=b\nonumber
\end{align}
where $P\in\symm_{+}^n$, $G\in\R^{m\times n}$, and $A\in\R^{p\times n}$.
A \textit{quadratically constrained quadratic program} (QCQP) has the form
\begin{align}
  \text{minimize\;\;}\quad& (1/2)x^TP_0x+q_0^Tx+r_0\nonumber\\
  \text{subject to}  \quad& (1/2)x^TP_ix+q_i^Tx+r_i\le 0,\;i=1,\dots,m\label{eq:4.35}\\
                          & Ax=b\nonumber
\end{align}
where $P_i\in\symm_{+}^n,\;i=0,\dots,m$.
\begin{itemize}
  \item QP with $P=0$ is LP
  \item QCQP with $P_i=0,\;i=1,\dots,m$ is QP
\end{itemize}


% 4.4.1
\subsection{Examples}

% 4.4.2
\subsection{Second-order cone programming}
\begin{example}[\textit{Portfolio optimization with loss risk constraints}]
\end{example}

% 4.5
\section{Geometric programming}

% 4.5.1
\subsection{Monomials and posynomials}

% 4.5.2
\subsection{Geometric programming}

% 4.5.3
\subsection{Geometric program in convex form}

% 4.5.4
\subsection{Examples}

% 4.6
\section{Generalized inequality constraints}

% 4.6.1
\subsection{Conic form problems}

% 4.6.2
\subsection{Semidefinite programming}

% 4.6.3
\subsection{Examples}

% 4.7
\section{Vector optimization}

% 4.7.1
\subsection{General and convex vector optimization problems}
\begin{example}[\textit{Best linear unbiased estimator}]
\end{example}

% 4.7.2
\subsection{Optimal points and values}

% 4.7.3
\subsection{Pareto optimal points and values}

% 4.7.4
\subsection{Scalarization}
\begin{example}[\textit{Minimal upper bound on a set of matrices}]
\end{example}

% 4.7.5
\subsection{Multicriterion optimization}

% 4.7.6
\subsection{Examples}
