%!TEX program = pdflatex
\documentclass[12pt]{extarticle}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{framed}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{mdframed}
\usepackage{blindtext}
\usepackage{hyperref}

\hypersetup{
  colorlinks=true,
  linkcolor=blue
}
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\everymath{\displaystyle}

\input{../defs.tex}

\title{Assignment \#3}
\author{Shao Hua, Huang 0750727\\
ECM5901 - Optimization Theory and Application}

\begin{document}
\maketitle

% Exercise 1
\begin{exercise}
  (Kullback-Leibler divergence and the information inequality.) The Kullback-Leibler divergence between $u,v\in\R_{++}^n$ is defined as
  \begin{align*}
    D_{kl}(u,v)=\sum_{i=1}^n(u_i\log(u_i/v_i)-u_i+v_i)
  \end{align*}
  Show that
  \begin{enumerate}[label=(\alph*)]
    \item $D_{kl}(u,v)$ is convex (5\%)
    \item $D_{kl}(u,v)\ge 0$ for all $u,v\in\R_{++}^n$ (5\%)
    \item $D_{kl}(u,v)=0$ if and only if $u=v$ (5\%)
  \end{enumerate}
  Hint: $D_{kl}(u,v)$ can be expressed as
  \begin{align}
    D_{kl}(u,v)=f(u)-f(v)-\nabla f(v)^T(u-v)\label{eq:1.1}
  \end{align}
  where $f(v)=\sum_{i=1}^nv_i\log v_i$ is the nagative entropy of $v$ for $v\in\R_{+}^n$.
\end{exercise}
\begin{proof}
  $ $
  \begin{enumerate}[label=(\alph*)]
    \item The \textit{negative logarithm} function $f(x)=-\log x$ on $\R_{++}$ is (strictly) convex.\\
          Apply \S 3.2.6, perspective of $f(x)$, we get \textit{relative entropy} function
          \begin{align}
            g_1(x,t)=t\log(t/x)\label{eq:1.2}
          \end{align}
          is (strictly) convex.\\
          Let $g_2(x,t)=-t+x$, then $g_2$ is an affine function (both convex and concave).\\
          Apply \S 3.2.1, nonnegative weighted sums of $g_1(x,t)$ and $g_2(x,t)$, we get
          \begin{align*}
            h(x,t)=g_1(x,t)+g_2(x,t)=t\log(t/x)-t+x
          \end{align*}
          is also convex.\\
          Apply \S 3.2.1 again, we can extends $h(x,t)$ to dimension $n$, therefore
          \begin{align*}
            D_{kl}(u,v)=\sum_{i=1}^n(u_i\log(u_i/v_i)-u_i+v_i)
          \end{align*}
          is convex.
    \item Substitute $x$ with $1$ in \eqref{eq:1.2} and extends the function to dimension $n$ by \S 3.2.1, we know that the \textit{negative entropy} function
          \begin{align*}
            f(v)=\sum_{i=1}^nv_i\log v_i
          \end{align*}
          is strictly convex. Therefore, for $u, v\in\dom f$ and $u\neq v$ we have
          \begin{align*}
            & f(u)>f(v)+\nabla f(v)^T(u-v)\\
            \Rightarrow & \sum_{i=1}^nu_i\log u_i>\sum_{i=1}^nv_i\log v_i+\sum_{i=1}^n(\log v_i+1)(u_i-v_i)\\
            \Rightarrow & \sum_{i=1}^nu_i\log u_i>\sum_{i=1}^nu_i\log v_i+\sum_{i=1}^n(u_i-v_i)\\
            \Rightarrow & \sum_{i=1}^n(u_i\log(u_i/v_i)-u_i+v_i)>0\\
            \Rightarrow & D_{kl}(u,v)>0
          \end{align*}
    \item $u=v\Rightarrow D_{kl}(u,v)=D_{kl}(u,u)=\sum_{i=1}^n(u_i\log(u_i/u_i)-u_i+u_i)=0$\\
          $D_{kl}(u,v)=0$ with $u\neq v$ contradicts to result of (b), \ie $D_{kl}(u,v)=0\Rightarrow u=v$
          Therefore, we have $u=v\Leftrightarrow D_{kl}(u,v)=0$\qedhere
  \end{enumerate}
\end{proof}
    
% Exercise 2
\begin{exercise}
  Adapt the proof of concavity of the log-determinant function in \S 3.1.5 of [BV04] to show that
  \begin{align*}
    f(X)=\tr (X^{-1})
  \end{align*}
  is convex on $\dom f=\symm_{++}^n$. (15\%)
\end{exercise}
\begin{proof}
  Consider an arbitrary line, given by $X=Z+tV$ where $Z, V\in\symm^n$.
  Define $g(t)=f(Z+tV)$, and restrict $g$ to the interval of values of $t$ for which $Z+tV\succ 0$.
  Without loss of generality, we can assume that $t=0$ is inside this interval, \ie $Z\succ 0$. We have
  \begin{align*}
    g(t) &= \tr ((Z+tV)^{-1})\\
         &= \tr (Z^{-1}(I+tZ^{-\frac{1}{2}}VZ^{-\frac{1}{2}})^{-1})\\
         &= \tr (Z^{-1}(I+tQ\Lambda Q^T)^{-1})\\
         &= \tr (Z^{-1}Q(I+t\Lambda)^{-1}Q^T)\\
         &= \tr (Q^TZ^{-1}Q(I+t\Lambda)^{-1})\\
         &= \sum_{i=1}^n (Q^TZ^{-1}Q)_{ii}(1+t\lambda_i)^{-1}
  \end{align*}
  where $Z^{-\frac{1}{2}}VZ^{-\frac{1}{2}}=Q\Lambda Q^T$ is the eigenvalue decomposition.\\
  Therefore, $g$ is nonnegative weighted sums of convex functions $(1+t\lambda_i)^{-1}$, \ie $g$ is convex.
\end{proof}

% Exercise 3
\begin{exercise}
  (Composition rules.) Show that the following functions are convex
  \begin{enumerate}[label=(\alph*)]
    \item $f(x,u,v)=-\log(uv-x^Tx)$ on\\
          $\dom f=\{(x,u,v)\mid x\in\R^n,\;u,v\in\R,\;uv>x^Tx,\;u,v>0\}$
    \item Show that
      \begin{align*}
        f(x)=\frac{\|Ax+b\|_2^2}{c^Tx+d}
      \end{align*}
      is convex on $\{x\in\R^n\mid c^Tx+d>0\}$, where $A\in\R^{m\times n}, b\in\R^m, c\in\R^n$ and $d\in\R$. (10\%)
  \end{enumerate}
  Hint: $x^Tx/u$ is convex in $(x,u)$ for $u>0$.
\end{exercise}
\begin{proof}
  $ $
  \begin{enumerate}[label=(\alph*)]
    \item Rewrite $f(x,u,v)=-\log u-\log(v-x^Tx/u)$.
          We know that $-\log u$ is convex, and $v-x^Tx/u$ is concave because $v$ linear and $x^Tx/u$ convex on $\{(x,u)\mid u>0\}$.\\
          By applying the rule of composition on $f(x)=h(g(x))$:
          $$f\text{ is convex if }h\text{ is convex and nonincreasing, and }g\text{ is concave}$$
          We get $-\log(v-x^Tx/u)$ is also convex.\\
          Therefore, $f(x,u,v)$ is convex (nonnegative weighted sums of convex functions).
    \item The function is composed by the convex function $g(y,t)=y^Ty/t$ and the affine mapping $f(x)=g(Ax+b,c^Tx+d)$.
          From \S 3.2.2 and $g(y,t)$ with $t>0$ is convex, we know that $f(x)$ is also convex.\qedhere
  \end{enumerate}
\end{proof}

% Exercise 4
\begin{exercise}
  (Conjugate of convex plus affine function) Define $g(x)=f(x)+c^Tx+d$ for $c\in\R^n,d\in\R$, where $f:\R^n\rightarrow\R$ is convex. Express $g^\ast$ in terms of $f^\ast$ and $c,\;d$. (10\%)
\end{exercise}
\begin{proof}[Solution]
  \let\qed\relax
  \begin{align*}
    g^\ast(y) &= \sup_{x\in\dom f} (y^Tx-f(x)-c^Tx-d)\\
              &= \sup_{x\in\dom f} ((y-c)^Tx-f(x))-d\\
              &= f^\ast(y-c)-d
  \end{align*}
\end{proof}

% Exercise 5
\begin{exercise}
  (Log-concavity of Gaussian cumulative distribution function.) The cumulative distribution function of a Gaussian random variable,
  \begin{align*}
    f(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-t^2/2}dt,
  \end{align*}
  is log-concave. (This follows from the general result that the convolution of two log-concave functions is log-concave.)
  In this problem we guide you through a simple self-contained proof that $f$ is log-concave.
  Recall that $f$ is log-concave if and only if $f''(x)f(x)\le f'(x)^2$ for all $x$. 
  \begin{enumerate}[label=(\alph*)]
    \item Verify that $f''(x)f(x)\le f'(x)^2$ for $x\ge 0$. That leaves us the hard part, which is to show the inequality for $x<0$. 
    \item Verify that for any $t$ and $x$, we have $t^2/2\ge -x^2/2+xt$. (Hint: consider the first order condition with the function $g(t) = t^2/2$.)
    \item Use part (b) to show that $e^{-t^2/2}\le e^{x^2/2-xt}$. Conclude that for $x<0$, 
      \begin{align*}
        \int_{-\infty}^xe^{-t^2/2}dt\le e^{x^2/2}\int_{-\infty}^xe^{-xt}dt.
      \end{align*}
    \item Use part (c) to verify that $f''(x)f(x)\le f'(x)^2$ for $x\le 0$.
  \end{enumerate}
  (20\%)
\end{exercise}
\begin{proof}
  $ $
  \begin{enumerate}[label=(\alph*)]
    \item $f(x)>0$, $f'(x)=\frac{e^{-x^2/2}}{\sqrt{2\pi}}>0$, and $f''(x)=-\frac{xe^{-x^2/2}}{\sqrt{2\pi}}\Rightarrow f''(x)\le 0$ for $x\ge 0$.\\
          Hence, $f''(x)f(x)\le f'(x)^2$ for $x\ge 0$
    \item $t^2/2$ is convex on $\R$, employ first-order condition, we have
          \begin{align}
            \frac{t^2}{2}\ge \frac{x^2}{2}+x(t-x)=-\frac{x^2}{2}+xt\label{eq:5.1}
          \end{align}
          The equation holds for any $x,t\in\R$.
    \item Take exponentials of \eqref{eq:5.1}, we have $e^{-t^2/2}\le e^{x^2/2-xt}$.\\
          Then take integrate, we have
          \begin{align}
            \int_{-\infty}^xe^{-t^2/2}dt
              & \le e^{x^2/2}\int_{-\infty}^xe^{-xt}dt\nonumber\\
              & = e^{x^2/2}\left(-\frac{e^{-x^2}}{x}\right)=-\frac{e^{-x^2/2}}{x}\label{eq:5.2}
          \end{align}
    \item The inequality $f''(x)f(x)\le f'(x)^2$ becomes
          \begin{align}
            & \left(-\frac{xe^{-x^2/2}}{\sqrt{2\pi}}\right)\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-t^2/2}dt\right)\le \left(\frac{e^{-x^2/2}}{\sqrt{2\pi}}\right)^2\nonumber\\
            \Rightarrow & -xe^{-x^2/2}\int_{-\infty}^xe^{-t^2/2}dt\le e^{-x^2}\nonumber\\
            \Rightarrow & \int_{-\infty}^xe^{-t^2/2}dt\le -\frac{e^{-x^2/2}}{x}\quad\text{(when }x<0\text{)}\label{eq:5.3}
          \end{align}
          \eqref{eq:5.2} and \eqref{eq:5.3} are actually the same inequality.\\
          Therefore, $f''(x)f(x)\le f'(x)^2$ holds for $x\le 0$ (it also holds trivially when $x=0$).\qedhere
  \end{enumerate}
\end{proof}

% Exercise 6
\begin{exercise}
  (Sublevel sets and epigraph of $K$-convex functions.) Let $K\subset \R^m$ be a proper convex cone with associated generalized inequality $K$, and let $f:\R^n\rightarrow\R^m$.
  For $\alpha\in\R^m$, the $\alpha$-sublevel set of $f$ (with respect to $K$) is defined as
  \begin{align*}
    C_\alpha=\{x\in\R^n\mid f(x)\preceq_{K}\alpha\}.
  \end{align*}
  The epigraph of $f$, with respect to $K$, is defined as the set
  \begin{align*}
    \epi_Kf=\{(x,t)\in\R^{n+m}\mid f(x)\preceq_Kt\}
  \end{align*}
  Show the following:
  \begin{enumerate}[label=(\alph*)]
    \item If $f$ is $K$-convex, then its sublevel sets $C_\alpha$ are convex for all $\alpha$. (10\%) 
    \item $f$ is $K$-convex if and only if $\epi_Kf$ is a convex set. (10\%)
  \end{enumerate}
\end{exercise}
\begin{proof}
  $ $
  \begin{enumerate}[label=(\alph*)]
    \item For all $x, y\in C_\alpha$, and $0\le\theta\le 1$,
          \begin{align*}
            f(\theta x+(1-\theta y))
              & \preceq_K \theta f(x)+(1-\theta)f(y) & \text{(}f\text{ is }K\text{-convex)}\\
              & \preceq_K \theta \alpha+(1-\theta)\alpha = \alpha & \text{(}x, y\in C_\alpha\text{)}
          \end{align*}
          Therefore, $C_\alpha$ is convex.
    \item Proposition of $\rightarrow$, $(x,u),(y,v)\in\epi_Kf$, and $0\le\theta\le 1$,
          \begin{align*}
            f(\theta x+(1-\theta y))
              & \preceq_K \theta f(x)+(1-\theta)f(y) & \text{(}f\text{ is }K\text{-convex)}\\
              & \preceq_K \theta u+(1-\theta)v & \text{(}(x,u),(y,v)\in\epi_Kf\text{)}
          \end{align*}
          $(\theta x+(1-\theta y),\theta u+(1-\theta v))\in\epi_Kf$, \ie $\epi_Kf$ is convex.\\
          Proposition of $\leftarrow$, let $f(x)=u,f(y)=v$ ($(x,u),(y,v)\in\epi_Kf$), and $0\le\theta\le 1$,
          \begin{align*}
            f(\theta x+(1-\theta y))
              & \preceq_K \theta u+(1-\theta)v & \text{(}\epi_Kf\text{ is convex)}\\
              & = \theta f(x)+(1-\theta)f(y)
          \end{align*}
          We get $\epi_Kf$ is $K$-convex.\\
          Therfore, $f$ is $K$-convex if and only if $\epi_Kf$ is convex.\qedhere
  \end{enumerate}
\end{proof}

\end{document}
